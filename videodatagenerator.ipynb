{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive')","metadata":{"id":"3X8CCOVEQZNG","outputId":"1d899b43-b676-4ae1-fe15-73aa3dfd9bf6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/metal3d/keras-video-generators.git  \n\n","metadata":{"id":"Zh1AMeGF-9Qi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras-video-generators\n","metadata":{"id":"PyAeektVCb0Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math   # for mathematical operations\nimport matplotlib.pyplot as plt    # for plotting the images\n%matplotlib inline\nimport pandas as pd\nfrom keras.preprocessing import image   # for preprocessing the images\nimport numpy as np    # for mathematical operations\nfrom keras.utils import np_utils\nfrom skimage.transform import resize   # for resizing images\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense, InputLayer, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.regularizers import l2\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras import datasets ,layers,models\n\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.metrics import classification_report\n\nfrom tensorflow.keras.models import Sequential\nfrom scipy.spatial import distance as dist\n#from imutils.video import FileVideoStream\nfrom scipy.signal import argrelextrema\n#from imutils.video import VideoStream\n#from keras_video import VideoFrameGenerator\n#from imutils import face_utils\nimport numpy as np\nimport argparse\n#import imutils\nimport time\nimport dlib\nimport cv2\nimport os\nimport glob\n\n\n# calculate a face embedding for each face in the dataset using facenet\nfrom numpy import load\nfrom numpy import expand_dims\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom keras.models import load_model\n\n# calculate a face embedding for each face in the dataset using facenet\nfrom numpy import load\nfrom numpy import expand_dims\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom keras.models import load_model","metadata":{"id":"QtTJM8lQm5VP","execution":{"iopub.status.busy":"2021-08-10T15:27:10.149908Z","iopub.execute_input":"2021-08-10T15:27:10.150225Z","iopub.status.idle":"2021-08-10T15:27:10.170572Z","shell.execute_reply.started":"2021-08-10T15:27:10.150195Z","shell.execute_reply":"2021-08-10T15:27:10.169499Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"### \"\"\"arr = os.listdir(\"/content/gdrive/MyDrive/DataBase\")\n#label_train  = np.array([])\n#Frame_train = np.array([])\ni = 0\nfor path_1 in arr :\n\n  \n  \n  #print(arr)\n  #verify if the path is a directory or just a hidden file\n  full_path_1 = f\"/content/gdrive/MyDrive/DataBase/{path_1}\" \n  isDirectory = os.path.isdir(full_path_1)\n  if isDirectory : \n    arr_1 = os.listdir(full_path_1)\n    \n    for path_2 in arr_1 : \n      full_path_2 = f\"/content/gdrive/MyDrive/DataBase/{path_1}/{path_2}\" \n      arr_2 = os.listdir(full_path_2)\n      \n      for path_3 in arr_2 :\n        i += 1 \n        label_train  = []\n        Frame_train = [] \n        #).reshape(0,1).astype(np.int64)\n        label = int(os.path.splitext(path_3)[0])\n        full_path_2 = f\"/content/gdrive/MyDrive/DataBase/{path_1}/{path_2}/{path_3}\"\n        \n        vs = cv2.VideoCapture(full_path_2)\n        while True :\n          ret, image = vs.read()\n          if ret == False : \n            break\n          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n          image = cv2.resize(image, (224, 224))\n          #image = image.img_to_array(image)\n          label_train.append(label)\n          Frame_train.append(image)\n        #train classes \n        np.save(f'/content/gdrive/MyDrive/variables/X_dataset{i}.npy', Frame_train) # save\n        #train labels\n        np.save(f'/content/gdrive/MyDrive/variables/y_dataset{i}.npy', label_train) # save\n        vs.release()\n        cv2.destroyAllWindows()\"\"\"","metadata":{"id":"esjgoJZ01Kkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#next step is to collect all the videos in one folder and take the first character of their name which is the label \nimport numpy as np\nimport keras\n#path to video's folder\npath = '/content/gdrive/MyDrive/variables/Fold2_part2'\n#\ntrain_ID = os.listdir(path)[1:]\nclass VideoFrameGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self,train_ID, batch_size=1, dim=(56,56), shuffle=True ):\n        'Initialization'\n        self.batch_size = batch_size\n        self.train_ID = train_ID   \n        self.shuffle = shuffle\n        self.dim = dim\n        self.on_epoch_end()\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.train_ID) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.train_ID[k] for k in indexes]\n        #list_IDs_temp = [20]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        X =  np.array([X])[0]\n        y =  np.array([y])\n        y = y.reshape(y.shape[1],1)\n        randomize = np.arange(y.shape[0])\n        np.random.shuffle(randomize)\n        X = X[randomize]\n        y = y[randomize]\n        return X , y \n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.train_ID))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        #y = np.empty((self.batch_size), dtype=int)\n        arr_1 = os.listdir(f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}')\n        label_train  = []\n        Frame_train = [] \n        # Generate data\n        for path_3 in arr_1 : \n          #).reshape(0,1).astype(np.int64)\n          label = int(os.path.splitext(path_3)[0])\n          full_path_2 = f\"/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}/{path_3}\"\n          vs = cv2.VideoCapture(full_path_2)\n          nb_frame = 0\n          while True :\n            ret, image = vs.read()\n            if ret == False  or nb_frame == 1000  : \n              break\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, self.dim)\n            #image = image.img_to_array(image)\n            label_train.append(label)\n            Frame_train.append(image)\n            nb_frame += 1\n          vs.release()\n          cv2.destroyAllWindows()\n        return Frame_train, label_train\n     \n","metadata":{"id":"ey_zVePI3a_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/content/gdrive/MyDrive/variables/Fold2_part2'\ntrain_ID = os.listdir(path)[1:]\n\n# Parameters\nparams = {'dim': (56,56),\n          'batch_size': 1,\n          'shuffle': True}\ntraining_generator = VideoFrameGenerator(train_ID, **params)\nvalidation_generator = VideoFrameGenerator([19], **params)\n ","metadata":{"id":"4WQjd3_uDlxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"model = tf.keras.models.Sequential([\n              tf.keras.layers.Conv2D(32, (3,3) , activation = 'relu' ,padding='same', input_shape = (56,56,3), kernel_regularizer=l2(0.01) ),\n              tf.keras.layers.MaxPool2D(pool_size=(2, 2) ),\n              #\n              tf.keras.layers.Conv2D(32 , (3,3) ,padding='same', activation = 'relu',kernel_regularizer=l2(0.01)),\n              tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n              #\n              tf.keras.layers.Conv2D(64 , (3,3) , activation = 'relu', padding='same', kernel_regularizer=l2(0.01)),\n              #tf.keras.layers.Dropout(0.2),\n              tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n              #\n            \n              #\n              #  tf.keras.layers.Conv2D(256 , (3,3) , activation = 'relu'),\n              #  tf.keras.layers.Dropout(0.2),\n              # tf.keras.layers.BatchNormalization() ,\n              # tf.keras.layers.MaxPool2D(2,2),\n              #\n              tf.keras.layers.Flatten(),\n              ##\n              tf.keras.layers.Dense(64 , activation = 'relu', kernel_regularizer=l2(0.01)),\n              #tf.keras.layers.Dropout(0.2),\n              tf.keras.layers.Dense(11,activation = 'softmax' ) ]\n               \n    )\nmodel.summary()\"\"\"\n","metadata":{"id":"AauwousQJoKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\nmodel.compile(optimizer = opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(training_generator, validation_data=validation_generator, epochs = 10 )\n#change 56 to 224 first thing to do \nhistory\"\"\"","metadata":{"id":"Bf0F1fymKKiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x ,y = training_generator.__getitem__(0)\n","metadata":{"id":"3LLvQ_K7g_Wi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##############################################################\n---\n\n\n\n---\n\n","metadata":{"id":"J_eXmU4LjcAg"}},{"cell_type":"code","source":"!pip install mtcnn","metadata":{"id":"g7QeXOoFgdu2","outputId":"4e0eb3c2-5902-47f7-a8fe-31443d7df56c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install gdown","metadata":{"id":"ZRXbkGEKgmYW","outputId":"b859085f-4a76-49f2-948c-d13126cea727","execution":{"iopub.status.busy":"2021-08-10T15:24:34.538019Z","iopub.execute_input":"2021-08-10T15:24:34.538361Z","iopub.status.idle":"2021-08-10T15:24:52.002990Z","shell.execute_reply.started":"2021-08-10T15:24:34.538324Z","shell.execute_reply":"2021-08-10T15:24:52.002010Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests[socks]>=2.12.0 in /opt/conda/lib/python3.7/site-packages (from gdown) (2.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.0.12)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.61.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9033 sha256=365aa01f538545e63686acfc5f2ad48c5a2bc49c6a2b525b9121e7de29fd7e38\n  Stored in directory: /root/.cache/pip/wheels/2f/2a/2f/86449b6bdbaa9aef873f68332b68be6bfbc386b9219f47157d\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-3.13.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# confirm mtcnn was installed correctly\nimport mtcnn\n# print version\nprint(mtcnn.__version__)","metadata":{"id":"ioQMCmWcgjVw","outputId":"765d1429-a839-4a45-c37a-40313569669f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model 1(facenet on tensorflow v1.x)\nimport gdown\n!gdown --id 1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1","metadata":{"id":"LfP21302hxp5","outputId":"13b3b034-dc10-429d-dd4a-db31e3f8359f","execution":{"iopub.status.busy":"2021-08-10T15:25:18.089426Z","iopub.execute_input":"2021-08-10T15:25:18.089755Z","iopub.status.idle":"2021-08-10T15:25:22.649360Z","shell.execute_reply.started":"2021-08-10T15:25:18.089725Z","shell.execute_reply":"2021-08-10T15:25:22.648130Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1\nTo: /kaggle/working/facenet_keras.h5\n92.4MB [00:00, 261MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1W_Kv4vKgKvGf0JU1zSYkvlv_1MpCmg5S","metadata":{"id":"sTSOmSXyAZCO","outputId":"44d15865-100f-488f-f6c6-b81a4247b39c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom numpy import asarray\nfrom mtcnn.mtcnn import MTCNN\n\n# extract a single face from a given photograph\ndef extract_face(pixels, required_size=(160, 160)): \n    # create the detector, using default weights   \n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    if len(results) == 0 : \n      return np.array([]).reshape(0, 160, 160, 3)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = asarray(image)\n    return face_array\n \n# load the photo and extract the face","metadata":{"id":"KrNBLordieH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_facenet = tf.keras.models.load_model('./facenet_keras.h5')\nprint('Loaded Model')","metadata":{"id":"TNp6tcQ-h0Oz","outputId":"2802bf57-1039-4f91-b32d-3b2d59cff4f3","execution":{"iopub.status.busy":"2021-08-10T15:48:42.578522Z","iopub.execute_input":"2021-08-10T15:48:42.578879Z","iopub.status.idle":"2021-08-10T15:48:45.232930Z","shell.execute_reply.started":"2021-08-10T15:48:42.578846Z","shell.execute_reply":"2021-08-10T15:48:45.231237Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Loaded Model\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model_facenet.inputs)\nprint(model_facenet.outputs)","metadata":{"id":"wQkZhjh7h2kh","outputId":"d91c0560-6d9e-4474-ac8b-c0be91285c4a","execution":{"iopub.status.busy":"2021-08-10T15:27:43.097942Z","iopub.execute_input":"2021-08-10T15:27:43.098288Z","iopub.status.idle":"2021-08-10T15:27:43.107254Z","shell.execute_reply.started":"2021-08-10T15:27:43.098257Z","shell.execute_reply":"2021-08-10T15:27:43.106300Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n","output_type":"stream"}]},{"cell_type":"code","source":"# get the face embedding for one face\ndef get_embedding(model_facenet , face_pixels):\n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) / std\n    # transform face into one sample\n    samples = expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    yhat = model_facenet.predict(samples)\n    return yhat[0]","metadata":{"id":"dI62bCi0iShx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### test code ","metadata":{"id":"IwxOfN1SkHDK"}},{"cell_type":"code","source":"arr_1 = os.listdir('/content/gdrive/MyDrive/variables/Fold3_part1/25')\ndetector = MTCNN()\n# Generate data\nfor path_3 in arr_1 : \n  label_train  = np.array([])\n  Frame_train = np.array([]).reshape(0,128)\n  #).reshape(0,1).astype(np.int64)\n  label = int(os.path.splitext(path_3)[0])\n  full_path_2 = f\"/content/gdrive/MyDrive/variables/Fold3_part1/25/{path_3}\"\n  #print(label)\n  vs = cv2.VideoCapture(full_path_2)\n  nb_frame = 0\n  #detector = MTCNN()\n\n  while True :\n    \n    ret, image = vs.read()\n    if ret == False or nb_frame == 3 : \n      break\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    #print(image.shape)\n    image = extract_face(image,required_size=(160, 160))\n    print(image.shape)\n    if len(image) == 0  :                \n        #face_features = np.array([]).reshape(0,128)\n        nb_frame += 1\n    else : \n        face_features = get_embedding(model_facenet , image)             \n        face_features = np.array(face_features) \n        face_features = face_features.reshape(1,128)\n        label_train = np.append(label_train , label)\n        Frame_train = np.append(Frame_train , face_features ,axis=0)\n        nb_frame += 1\n    #image = image.img_to_array(image)\n    #label_train = np.append(label_train , label)\n    #Frame_train = np.append(Frame_train , face_features ,axis=0)\n     \n  vs.release()\n  cv2.destroyAllWindows()","metadata":{"id":"lieRQsP7iWmv","outputId":"cf327007-b002-4a4d-a90d-2d55a57c8cb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#next step is to collect all the videos in one folder and take the first character of their name which is the label \n#path to videos folder\npath = '/content/gdrive/MyDrive/variables/Fold2_part2'\n#list of subfolders ID, os.listdir(path)[0] is for the validation data\ntrain_ID = os.listdir(path)[1:]\nclass VideoFrameGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self,train_ID , model_facenet ,detector = MTCNN(), batch_size=1, dim=(56,56), shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.train_ID = train_ID   \n        self.shuffle = shuffle\n        self.model_facenet = model_facenet\n        self.dim = dim\n        self.on_epoch_end()\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.train_ID) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.train_ID[k] for k in indexes]\n        #list_IDs_temp = [20]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        #X =  np.array([X])[0]\n        #y =  np.array([y])\n        #y = y.reshape(y.shape[1],1)\n        print(X.shape)\n        print(y.shape)\n        return X , y \n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.train_ID))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        #y = np.empty((self.batch_size), dtype=int)\n        arr_1 = os.listdir(f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}')\n        # Generate data\n        label_train  = np.array([]).reshape(0,1)\n        Frame_train = np.array([]).reshape(0,128)\n        for path_3 in arr_1 :\n            #labeling the data using the title of videos \n            label = int(os.path.splitext(path_3)[0])\n            full_path_2 = f'/content/gdrive/MyDrive/variables/Fold2_part2/{list_IDs_temp[0]}/{path_3}'\n            # Opens the Video file\n            \n            vs = cv2.VideoCapture(full_path_2)\n            nb_frame = 1        \n            while True :    \n                ret, image = vs.read()\n                if ret == False or nb_frame == 3 : \n                    break\n                \n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                #print(image.shape)\n                image = extract_face(image,required_size=(160, 160))\n                #image = cv2.resize(image, (160,160))\n                if not image :                \n                    #face_features = np.array([]).reshape(0,128)\n                    nb_frame += 1 \n                else : \n                    face_features = get_embedding(self.model_facenet , image)             \n                    face_features = np.array(face_features) \n                    face_features = face_features.reshape(1,128)\n                    label_train = np.append(label_train , label)\n                    Frame_train = np.append(Frame_train , face_features ,axis=0)\n                    nb_frame += 1 \n                #image = image.img_to_array(image)\n                #label_train = np.append(label_train , label)\n                #Frame_train = np.append(Frame_train , face_features ,axis=0)\n                \n            vs.release()\n            cv2.destroyAllWindows()\n        return Frame_train, label_train.reshape(label_train.shape[0]).astype(int)\n     \n","metadata":{"id":"qVi5BRfTI11H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/content/gdrive/MyDrive/variables/Fold2_part2'\ntrain_ID = os.listdir(path)[1:]\n\n# Parameters\nparams = {'model_facenet' : model_facenet , \n          'batch_size': 1,\n          'shuffle': True}\ntraining_generator = VideoFrameGenerator(train_ID, **params)\nvalidation_generator = VideoFrameGenerator([os.listdir(path)[0]], **params)\n ","metadata":{"id":"CfkOqeWHRlnj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x ,y = training_generator.__getitem__(0)\n","metadata":{"id":"X8waLLO04aaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"id":"_SsB8BTL5OQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = models.Sequential([\n          layers.Dense(256,activation='relu'),\n          layers.Dense(128,activation='relu'),\n          layers.Dense(64,activation='relu'),\n          layers.Dense(32,activation='relu'),\n          layers.Dense(16,activation='relu'),\n          layers.Dense(11,activation='softmax'),\n\n])\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\nmodel.compile(optimizer=opt,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nhistory = model.fit(training_generator , validation_data= validation_generator, epochs = 2 )\nhistory","metadata":{"id":"1DaFjMdyenBk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## at the part below , I used the Multi-task Cascaded Convolutional Networks (MTCNN) to detect faces in frames and then detect features from those Faces using the FaceNet model , which takes too long comparing to Dlib method(which i'm going to try ) , so the main purpose here is to compare the result(using whole face features) to the one we already found using the eye_aspect_ratio \nps : I will not use a sequence model\n\n","metadata":{"id":"akbFgP5AFPs1"}},{"cell_type":"code","source":"! pip install gdown","metadata":{"execution":{"iopub.status.busy":"2021-08-10T23:11:20.520316Z","iopub.execute_input":"2021-08-10T23:11:20.520710Z","iopub.status.idle":"2021-08-10T23:11:38.016960Z","shell.execute_reply.started":"2021-08-10T23:11:20.520610Z","shell.execute_reply":"2021-08-10T23:11:38.016034Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests[socks]>=2.12.0 in /opt/conda/lib/python3.7/site-packages (from gdown) (2.25.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.15.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.0.12)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.61.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (2021.5.30)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.26.5)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\nBuilding wheels for collected packages: gdown\n  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9033 sha256=1b3d32f016c24132bd43e6bebff5c0b2137cc8f478aa77a45e8d9cd60be90b4a\n  Stored in directory: /root/.cache/pip/wheels/2f/2a/2f/86449b6bdbaa9aef873f68332b68be6bfbc386b9219f47157d\nSuccessfully built gdown\nInstalling collected packages: gdown\nSuccessfully installed gdown-3.13.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"#model 1(facenet on tensorflow v1.x)\nimport gdown\n!gdown --id 1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1","metadata":{"execution":{"iopub.status.busy":"2021-08-10T23:11:38.020279Z","iopub.execute_input":"2021-08-10T23:11:38.020549Z","iopub.status.idle":"2021-08-10T23:11:43.156038Z","shell.execute_reply.started":"2021-08-10T23:11:38.020521Z","shell.execute_reply":"2021-08-10T23:11:43.155173Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1PZ_6Zsy1Vb0s0JmjEmVd8FS99zoMCiN1\nTo: /kaggle/working/facenet_keras.h5\n92.4MB [00:00, 229MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import math   # for mathematical operations\nimport matplotlib.pyplot as plt    # for plotting the images\n%matplotlib inline\nimport pandas as pd\nfrom keras.preprocessing import image   # for preprocessing the images\nimport numpy as np    # for mathematical operations\nfrom keras.utils import np_utils\nfrom skimage.transform import resize   # for resizing images\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom tqdm import tqdm\n#from google.colab.patches import cv2_imshow\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense, InputLayer, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.regularizers import l2\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras import datasets ,layers,models\n\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.metrics import classification_report\n\nfrom tensorflow.keras.models import Sequential\nfrom scipy.spatial import distance as dist\n#from imutils.video import FileVideoStream\nfrom scipy.signal import argrelextrema\n#from imutils.video import VideoStream\n#from keras_video import VideoFrameGenerator\n#from imutils import face_utils\nimport numpy as np\nimport argparse\n#import imutils\nimport time\nimport dlib\nimport cv2\nimport os\nimport glob\n\n\n# calculate a face embedding for each face in the dataset using facenet\nfrom numpy import load\nfrom numpy import expand_dims\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom keras.models import load_model\n\n# calculate a face embedding for each face in the dataset using facenet\nfrom numpy import load\nfrom numpy import expand_dims\nfrom numpy import asarray\nfrom numpy import savez_compressed\nfrom keras.models import load_model","metadata":{"id":"9twyndh4GbwC","execution":{"iopub.status.busy":"2021-08-10T23:11:45.724452Z","iopub.execute_input":"2021-08-10T23:11:45.724821Z","iopub.status.idle":"2021-08-10T23:11:51.250893Z","shell.execute_reply.started":"2021-08-10T23:11:45.724786Z","shell.execute_reply":"2021-08-10T23:11:51.250069Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nmodel_facenet = tf.keras.models.load_model('./facenet_keras.h5')\nprint('Loaded Model')","metadata":{"execution":{"iopub.status.busy":"2021-08-10T23:12:29.633166Z","iopub.execute_input":"2021-08-10T23:12:29.633484Z","iopub.status.idle":"2021-08-10T23:12:34.367517Z","shell.execute_reply.started":"2021-08-10T23:12:29.633455Z","shell.execute_reply":"2021-08-10T23:12:34.366647Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Loaded Model\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget   http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n","metadata":{"id":"4Q8OwcWJaBqW","outputId":"3039982f-101f-4e8d-e18a-c84072ea0926","execution":{"iopub.status.busy":"2021-08-10T23:12:34.371530Z","iopub.execute_input":"2021-08-10T23:12:34.373565Z","iopub.status.idle":"2021-08-10T23:12:37.682908Z","shell.execute_reply.started":"2021-08-10T23:12:34.373522Z","shell.execute_reply":"2021-08-10T23:12:37.682033Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"--2021-08-10 23:12:34--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\nResolving dlib.net (dlib.net)... 107.180.26.78\nConnecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 64040097 (61M)\nSaving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n\nshape_predictor_68_ 100%[===================>]  61.07M  26.1MB/s    in 2.3s    \n\n2021-08-10 23:12:37 (26.1 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!bunzip2 ./shape_predictor_68_face_landmarks.dat.bz2\n","metadata":{"id":"ISiAIdJ2caXI","outputId":"c47d4dd7-f02e-4609-a00a-7d5f67336c34","execution":{"iopub.status.busy":"2021-08-10T23:12:37.686202Z","iopub.execute_input":"2021-08-10T23:12:37.686478Z","iopub.status.idle":"2021-08-10T23:12:44.733554Z","shell.execute_reply.started":"2021-08-10T23:12:37.686449Z","shell.execute_reply":"2021-08-10T23:12:44.732481Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/davisking/dlib-models/raw/master/mmod_human_face_detector.dat.bz2","metadata":{"id":"al_Q6bwbi3Sc","outputId":"34edae09-1219-4a81-bf31-894c578bd1e2","execution":{"iopub.status.busy":"2021-08-10T23:12:44.737359Z","iopub.execute_input":"2021-08-10T23:12:44.737630Z","iopub.status.idle":"2021-08-10T23:12:46.154485Z","shell.execute_reply.started":"2021-08-10T23:12:44.737601Z","shell.execute_reply":"2021-08-10T23:12:46.153635Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"--2021-08-10 23:12:45--  https://github.com/davisking/dlib-models/raw/master/mmod_human_face_detector.dat.bz2\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/davisking/dlib-models/master/mmod_human_face_detector.dat.bz2 [following]\n--2021-08-10 23:12:45--  https://raw.githubusercontent.com/davisking/dlib-models/master/mmod_human_face_detector.dat.bz2\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 694687 (678K) [application/octet-stream]\nSaving to: ‘mmod_human_face_detector.dat.bz2’\n\nmmod_human_face_det 100%[===================>] 678.41K  --.-KB/s    in 0.03s   \n\n2021-08-10 23:12:46 (21.0 MB/s) - ‘mmod_human_face_detector.dat.bz2’ saved [694687/694687]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!bunzip2 ./mmod_human_face_detector.dat.bz2","metadata":{"id":"lJ-NYfl5jFvB","outputId":"ab9fcbe3-e7fe-4aea-a619-1e99fd1be8f8","execution":{"iopub.status.busy":"2021-08-10T23:12:46.156696Z","iopub.execute_input":"2021-08-10T23:12:46.157072Z","iopub.status.idle":"2021-08-10T23:12:46.874133Z","shell.execute_reply.started":"2021-08-10T23:12:46.157026Z","shell.execute_reply":"2021-08-10T23:12:46.873065Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"detector = dlib.get_frontal_face_detector()","metadata":{"id":"RPLYgv-Ccd2Z","outputId":"b6d47d45-b466-4cf1-cf88-fa9a36f3ad92","execution":{"iopub.status.busy":"2021-08-10T23:12:46.877469Z","iopub.execute_input":"2021-08-10T23:12:46.877761Z","iopub.status.idle":"2021-08-10T23:12:47.362289Z","shell.execute_reply.started":"2021-08-10T23:12:46.877731Z","shell.execute_reply":"2021-08-10T23:12:47.361335Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# get the face embedding for one face\ndef get_embedding(model_facenet , face_pixels):\n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) / std\n    # transform face into one sample\n    samples = expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    yhat = model_facenet.predict(samples)\n    return yhat[0]","metadata":{"id":"iJfgjmOZBuXM","execution":{"iopub.status.busy":"2021-08-10T23:12:47.363845Z","iopub.execute_input":"2021-08-10T23:12:47.364379Z","iopub.status.idle":"2021-08-10T23:12:47.373534Z","shell.execute_reply.started":"2021-08-10T23:12:47.364340Z","shell.execute_reply":"2021-08-10T23:12:47.372514Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#next step is to collect all the videos in one folder and take the first character of their name which is the label \n#path to videos folder\n#path = '/content/gdrive/MyDrive/variables/Fold2_part2'\n#list of subfolders ID, os.listdir(path)[0] is for the validation data\n#train_ID = os.listdir(path)[1:]\nclass VideoFrameGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self,train_ID ,model_facenet,  batch_size=1, dim=(56,56), shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.train_ID = train_ID   \n        self.shuffle = shuffle\n        self.model_facenet = model_facenet\n        self.dim = dim\n        self.on_epoch_end()\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.train_ID) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # Find list of IDs\n        list_IDs_temp = [self.train_ID[k] for k in indexes]\n        #list_IDs_temp = [20]\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n        #X =  np.array([X])[0]\n        #y =  np.array([y])\n        #y = y.reshape(y.shape[1],1)\n        return X , y \n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.train_ID))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        #X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        #y = np.empty((self.batch_size), dtype=int)\n        arr_1 = os.listdir(f'../input/fold2-part2/Fold2_part2/Fold2_part2/{list_IDs_temp[0]}')\n        # Generate data\n        label_train  = np.array([]).reshape(0,1)\n        Frame_train = np.array([]).reshape(0,128)\n        for path_3 in arr_1 :  \n            #labeling the data using the title of videos \n            label = int(os.path.splitext(path_3)[0])\n            full_path_2 = f'../input/fold2-part2/Fold2_part2/Fold2_part2/{list_IDs_temp[0]}/{path_3}'\n            # Opens the Video file\n            vs = cv2.VideoCapture(full_path_2)\n            nb_frame = 1  \n            while True : \n                 \n                ret, image = vs.read()\n                if ret == False  or nb_frame ==  5000 : \n                    break      \n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                  #print(image.shape)\n                rects = detector(image, 0)\n                for face in rects:\n                    x1 = face.left() # left point\n                    y1 = face.top() # top point\n                    x2 = face.right() # right point\n                    y2 = face.bottom() # bottom point\n                    img = image[y1:y2,x1:x2]\n                    img = cv2.resize(image, (160,160))\n                    \n                    #if  img.size == 0 :\n                        #print(nb_frame)\n                        #face_features = np.array([]).reshape(0,128)\n                       # nb_frame += 1 \n                    if img.size != 0 :          \n                        face_features = get_embedding(self.model_facenet , img)                \n                        face_features = np.array(face_features) \n                        face_features = face_features.reshape(1,128)\n                        label_train = np.append(label_train , label)\n                        Frame_train = np.append(Frame_train , face_features ,axis=0)\n                        nb_frame += 1 \n                #image = image.img_to_array(image)\n                #label_train = np.append(label_train , label)\n                #Frame_train = np.append(Frame_train , face_features ,axis=0)\n              \n            #vs.release()\n            #cv2.destroyAllWindows()\n        return Frame_train, label_train.reshape(label_train.shape[0]).astype(int)\n     ","metadata":{"id":"MSUAUwckcju0","execution":{"iopub.status.busy":"2021-08-10T23:38:29.616992Z","iopub.execute_input":"2021-08-10T23:38:29.617535Z","iopub.status.idle":"2021-08-10T23:38:29.655974Z","shell.execute_reply.started":"2021-08-10T23:38:29.617485Z","shell.execute_reply":"2021-08-10T23:38:29.654749Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"path = '../input/fold2-part2/Fold2_part2/Fold2_part2'\narr_Id = os.listdir(path)\ntrain_ID = arr_Id[1:]\n# Parameters\nparams = {'model_facenet' : model_facenet , \n          'batch_size': 1,\n          'shuffle': True}\ntraining_generator = VideoFrameGenerator(train_ID, **params)\nvalidation_generator = VideoFrameGenerator([arr_Id[0]], **params)","metadata":{"id":"7WQ1pE6pYFUw","execution":{"iopub.status.busy":"2021-08-10T23:38:32.031356Z","iopub.execute_input":"2021-08-10T23:38:32.031672Z","iopub.status.idle":"2021-08-10T23:38:32.041900Z","shell.execute_reply.started":"2021-08-10T23:38:32.031640Z","shell.execute_reply":"2021-08-10T23:38:32.040948Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"x ,y = training_generator.__getitem__(0)\n","metadata":{"id":"TR0kBvOQYgdB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.Sequential([\n          layers.Dense(256,activation='relu'),\n          layers.Dense(128,activation='relu'),\n          layers.Dense(64,activation='relu'),\n          layers.Dense(32,activation='relu'),\n          layers.Dense(16,activation='relu'),\n          layers.Dense(11,activation='softmax'),\n\n])\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\ncheckpoint_path = \"./cp.ckpt\"\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True, save_weights_only=True, verbose=1,save_freq=1)\n\nmodel.compile(optimizer=opt,\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\nhistory = model.fit(training_generator , validation_data= validation_generator, epochs = 10, callbacks=[cp_callback] )\nhistory","metadata":{"id":"0bJoNLMzebyf","outputId":"f4e10347-eb34-48af-e991-6d6f8174ebf0","execution":{"iopub.status.busy":"2021-08-10T23:38:38.671114Z","iopub.execute_input":"2021-08-10T23:38:38.671444Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1/5 [=====>........................] - ETA: 4:38:01 - loss: 2.3133 - accuracy: 0.3330","output_type":"stream"}]},{"cell_type":"code","source":"#load pre-trained weights\nmodel.load_weights(checkpoint_path)\nhistory = model.fit(training_generator , validation_data= validation_generator, epochs = 10, callbacks=[cp_callback] )\nhistory","metadata":{"id":"3Ueom2ODektq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8LwhvXeseqRC"},"execution_count":null,"outputs":[]}]}